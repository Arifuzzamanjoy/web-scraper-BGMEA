{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>web-scraper-BGMEA-Contact</h1>\n",
    "<p>A Python script to scrape the BGMEA member list from their website. It uses Selenium, Requests, BeautifulSoup, and Pandas.<br> The output is a CSV file with various information about each member company. For educational purposes only.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Features</h3>\n",
    "<p>Scrapes the name, registration number, contact person, email address, and mobile number of each member company from the BGMEA website.<br>\n",
    "Accesses the individual member page for each company and extracts additional information from the table element.<br>\n",
    "Handles exceptions such as missing table element or timeout error using try-except blocks.<br>\n",
    "Saves the scraped data in a CSV file with appropriate column names.<br><p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Requirements</h3>\n",
    "Python 3.x<br>\n",
    "Selenium<br>\n",
    "Requests<br>\n",
    "BeautifulSoup<br>\n",
    "Pandas<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Usage</h3>\n",
    "<p>Clone or download this repository to your local machine.<br>\n",
    "Install the required libraries using pip install -r requirements.txt.<br>\n",
    "Open the scraper.py file and change the num_pages variable to the number of pages you want to scrape. The default value is 370,<br> which corresponds to the total number of pages on the BGMEA website as of September 2023.<br>\n",
    "Run the script using python scraper.py.<br>\n",
    "Wait for the script to finish scraping and check the out.csv file for the output.<br></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "# Import TimeoutException and StaleElementReferenceException\n",
    "from selenium.common.exceptions import TimeoutException, StaleElementReferenceException\n",
    "\n",
    "# Create an empty DataFrame to store the data\n",
    "df = pd.DataFrame(columns=['Member/Company Name', 'BGMEA Reg No', 'Contact Person', 'Email Address', 'Mobile'])\n",
    "# Define the base URL\n",
    "base_url = 'https://bgmea.com.bd/page/member-list?page='\n",
    "# Define the number of pages to scrape\n",
    "num_pages = 370 # You can change this to any number you want\n",
    "\n",
    "# Loop over the page numbers\n",
    "for i in range(1, num_pages + 1):\n",
    "    # Append the page number to the base URL\n",
    "    url = base_url + str(i)\n",
    "    # Use Selenium to get the HTML content of the page\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    # Wait for the table to load\n",
    "    table = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, '.table'))\n",
    "    )\n",
    "    \n",
    "    #table = WebDriverWait(driver, 10).until(\n",
    "    #EC.presence_of_element_located((By.CSS_SELECTOR, '.table'))\n",
    "    #)\n",
    "    #\n",
    "    # Get the current window handle\n",
    "    window_before = driver.current_window_handle\n",
    "    # Loop through each row in the table\n",
    "    for j, row in enumerate(table.find_elements(By.CSS_SELECTOR, 'tbody tr'), start=1):\n",
    "        # Use a try-catch block to handle exceptions\n",
    "        try:\n",
    "            # Extract the text from each cell using explicit wait and a different locator\n",
    "            name = row.find_element(By.XPATH, './td[1]').text\n",
    "            reg = row.find_element(By.XPATH, './td[2]').text\n",
    "            contact = row.find_element(By.XPATH, './td[3]').text\n",
    "            email = row.find_element(By.XPATH, './td[4]').text\n",
    "            \n",
    "            \n",
    "            # Access another site with the j variable\n",
    "            url = \"https://www.bgmea.com.bd/member/\"+str(j)\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            \n",
    "            # Open a new window or tab with the url\n",
    "            driver.execute_script(\"window.open('');\")\n",
    "            \n",
    "            # Switch to the new window or tab\n",
    "            driver.switch_to.window(driver.window_handles[1])\n",
    "            \n",
    "            # Go to the url\n",
    "            driver.get(url)\n",
    "            \n",
    "            # Find the table element by its class name\n",
    "            try:\n",
    "                table=soup.find(\"table\", class_=\"table-bordered\")\n",
    "                \n",
    "                # Find all the rows in the table\n",
    "                rows = table.find_all(\"tr\")\n",
    "                \n",
    "                # Create an empty list to store the data\n",
    "                data = []\n",
    "                \n",
    "                # Loop through each row\n",
    "                for row in rows:\n",
    "                    # Find all the cells in the row\n",
    "                    cells = row.find_all(\"td\")\n",
    "                    # If there are four cells in the row\n",
    "                    if len(cells) == 4:\n",
    "                        # Extract the position, name, mobile number, and email from each cell\n",
    "                        mobile = cells[2].text.strip()\n",
    "                        \n",
    "                        # Append a tuple of the data to the list\n",
    "                        data.append((mobile))\n",
    "                        \n",
    "                # Print the list of data\n",
    "                #print(data)\n",
    "                \n",
    "            except AttributeError:\n",
    "                # Skip the web page if the table element is not found\n",
    "                print(\"No table element found for url: \" + url)\n",
    "                continue\n",
    "            \n",
    "            # Close the current window or tab\n",
    "            driver.close()\n",
    "            \n",
    "            # Switch back to the original window or tab\n",
    "            driver.switch_to.window(window_before)\n",
    "            \n",
    "            # Merge data and df\n",
    "            df = df.append({'Member/Company Name': name, 'BGMEA Reg No': reg, 'Contact Person': contact, 'Email Address': email, 'Mobile': data}, ignore_index=True)    \n",
    "        \n",
    "        except (TimeoutException, StaleElementReferenceException):\n",
    "            # Retry finding or interacting with the element if an exception occurs\n",
    "            print(\"Exception occurred for url: \" + url)\n",
    "            continue\n",
    "    \n",
    "    # Quit the browser after scraping one page\n",
    "    driver.quit()\n",
    "\n",
    "# Print or save df as you wish            \n",
    "print(df)\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "df.to_csv('bgmeaContact.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
